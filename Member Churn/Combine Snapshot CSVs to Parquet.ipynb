{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converts hundreds of CSVs to a small number of Parquet files so they can be uploaded more efficiently to Snowflake and then combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install dask[dataframe] # install dask library in terminal for dataframe handling\\npip install parquet-tools # Used to view content of parquet files in CLI\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pip install dask[dataframe] # install dask library in terminal for dataframe handling\n",
    "pip install parquet-tools # Used to view content of parquet files in CLI\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import dask.dataframe as dd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = ### #change to your folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of files in folder pathway\n",
    "total_files = 0\n",
    "files_in_folder = os.listdir(folder_path)\n",
    "csv_files = [file for file in files_in_folder if file.endswith('.csv')]\n",
    "csv_file_paths = [os.path.join(folder_path, file) for file in csv_files]\n",
    "df = dd.read_csv(csv_file_paths, dtype=str)\n",
    "for file in csv_file_paths:\n",
    "  total_files += 1\n",
    "print(f\"Number of files: {total_files}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact Backup December 01 2024.csv\n",
      "Contact Backup February 01 2025.csv\n",
      "Contact Backup January 01 2025.csv\n",
      "Contact Backup November 02 2024.csv\n",
      "Contact Backup October 02 2024.csv\n"
     ]
    }
   ],
   "source": [
    "#List all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "#List to store individual DataFrames\n",
    "dask_dfs = []\n",
    "\n",
    "#Loop over each CSV file\n",
    "for file in csv_files:\n",
    "  try:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "\n",
    "    #Read the CSV file into a Dask DataFrame\n",
    "    df = dd.read_csv(file_path, dtype=str, on_bad_lines='skip', engine='python')\n",
    "\n",
    "    print(os.path.basename(file))\n",
    "\n",
    "    #Add a new column with the filename\n",
    "    basename = os.path.basename(file)\n",
    "    df['SNAPSHOT_DATE'] = pd.to_datetime(basename.replace('Contact Backup ','').replace('.csv', '')) # Replace file name to only have the date\n",
    "\n",
    "    #Append the DataFrame to the list\n",
    "    dask_dfs.append(df)\n",
    "  except Exception as e:\n",
    "    print(f\"Error reading file: {file}, Error: {e}\")\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate all DataFrames into a single Dask DataFrame\n",
    "dask_df = dd.concat(dask_dfs, axis=0)\n",
    "\n",
    "#Display the first few rows to test if SNAPSHOT_DATE column was added\n",
    "dask_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = dask_df.shape[0].compute() // 3 #Counts how many rows are in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866542\n"
     ]
    }
   ],
   "source": [
    "print(num_rows) #Use this number to help determine the number of parquet files wanted, it's suggested that a single file shouldn't hold more than 2,000,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE npartitions to number of parquet files you want as output\n",
    "df_repartitioned = dask_df.repartition(npartitions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the Dask DataFrame to parquet files\n",
    "parquet_path = ##\n",
    "df_repartitioned.to_parquet(parquet_path, write_index=False) #Change to output pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet-tools show [file pathway] # Use command in terminal to view content of parquet file. Change [file pathway] to your file path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
