{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following automates the upload of all files in a local folder to Snowflake tables via staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install snowflake-connector-python pandas python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from snowflake.connector import connect\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to CSV folder\n",
    "csv_folder = os.getenv(\"FOLDER_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "#Establish Snowflake connection\n",
    "conn = connect(\n",
    "    user=os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    password=os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    account=os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    warehouse=os.getenv(\"WAREHOUSE_NAME\"),\n",
    "    database=os.getenv(\"DATABASE_NAME\"),\n",
    "    schema=os.getenv(\"SCHEMA_NAME\")\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "#Loop through all CSVs\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if not filename.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(csv_folder, filename)\n",
    "    table_name = os.path.splitext(filename)[0].upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "    print(f\"Processing: {filename}\")\n",
    "\n",
    "    #Preview file with pandas to determine column types\n",
    "    try:\n",
    "        df_sample = pd.read_csv(filepath, nrows=100, encoding='utf-8', on_bad_lines='skip')\n",
    "        print(f\"{filename} loaded with UTF-8 encoding.\")\n",
    "    except UnicodeDecodeError:\n",
    "        df_sample = pd.read_csv(filepath, nrows=100, encoding='windows-1252', on_bad_lines='skip')\n",
    "        print(f\"{filename} loaded with Windows-1252 encoding.\")\n",
    "        \n",
    "    #Build CREATE TABLE query, make sure column datatypes are parsed correctly\n",
    "    col_defs = []\n",
    "    for col, dtype in zip(df_sample.columns, df_sample.dtypes):\n",
    "        col_sql = col.upper().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        if pd.api.types.is_integer_dtype(dtype):\n",
    "            sql_type = \"NUMBER\"\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            sql_type = \"FLOAT\"\n",
    "        elif pd.api.types.is_bool_dtype(dtype):\n",
    "            sql_type = \"BOOLEAN\"\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "            sql_type = \"TIMESTAMP_NTZ\"\n",
    "        else:\n",
    "            sql_type = \"VARCHAR\"\n",
    "        col_defs.append(f'\"{col_sql}\" {sql_type}')\n",
    "    \n",
    "    create_stmt = f'CREATE OR REPLACE TABLE \"MARKETING_CLOUD_EXTRACTS\".\"{table_name}\" ({\", \".join(col_defs)});' #each table name points to new Snowflake table in schema\n",
    "    cur.execute(create_stmt)\n",
    "    print(f\"✅ Created table: {table_name}\") #check, icon is unnecessary but helps readability\n",
    "\n",
    "    #Upload to internal stage\n",
    "    escaped_path = filepath.replace(\"\\\\\", \"/\")  #deal with annoying slash problem\n",
    "    put_stmt = f\"PUT 'file://{escaped_path}' @MC_EXTRACT_STAGE AUTO_COMPRESS=TRUE OVERWRITE=TRUE\"\n",
    "    cur.execute(put_stmt) #execute the query\n",
    "    print(f\"Uploaded to stage: {filename}\") #check that file was uploaded to stage\n",
    "\n",
    "    #COPY INTO table\n",
    "    copy_stmt = f\"\"\"\n",
    "        COPY INTO MARKETING_CLOUD_EXTRACTS.{table_name}\n",
    "        FROM @MC_EXTRACT_STAGE/{filename}.gz\n",
    "        FILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1 FIELD_OPTIONALLY_ENCLOSED_BY = '\"')\n",
    "        ON_ERROR = 'CONTINUE';\n",
    "    \"\"\"\n",
    "    cur.execute(copy_stmt)\n",
    "    print(f\"✅ Loaded into table: {table_name}\\n\") \n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "print(\"All files uploaded and loaded successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
